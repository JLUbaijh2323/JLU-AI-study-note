---
title: CUDA 编程入门极简教程 - 精品笔记
author: AI小小将
source: https://zhuanlan.zhihu.com/p/34587739 (假设的原始链接)
tags:
  - CUDA
  - GPU
  - 并行计算
  - 编程入门
  - GPGPU
aliases: [CUDA 笔记, GPU 编程基础]
creation_date: 2025-11-13
---

# CUDA 编程入门极简教程 - 学习笔记

> [!summary] 核心概要
> 这篇笔记详细拆解了原文《CUDA 编程入门极简教程》，旨在建立一个从零到一的 CUDA 编程知识框架。CUDA 的本质是**利用 GPU 众多的核心来实现大规模数据并行计算**。
> 
> 学习 CUDA 的关键在于理解其**异构计算模型**（CPU + GPU 协同）和**线程管理模型**（Grid / Block / Thread 的三层逻辑结构如何映射到物理硬件 SM / Warp）。

## 💡 我的思考与提炼 (Key Takeaways)

1.  **模型的“翻译”是核心：** 学习 CUDA 最重要的一点，是理解如何将一个**逻辑上的并行问题**（例如“给这个100万元素的向量每个值加1”）“翻译”成 GPU 能理解的**物理执行单元**。
    * **你的“翻译”：** `kernel<<<grid, block>>>`。你通过 Grid 和 Block 的维度设置，告诉 GPU 你想启动多少个线程。
    * **GPU 的“执行”：** GPU 的 SM（流式多处理器）接收这些“Block”（线程块），并将其拆分为 32 个线程一组的 **Warp**（线程束）来实际执行。
2.  **两个“世界”，两种内存：** 编程时必须时刻分清**Host (CPU)** 和 **Device (GPU)**。
    * **Host (主机端):** 负责逻辑控制、串行任务、数据准备。
    * **Device (设备端):** 负责并行计算、数据密集型任务。
    * **内存“鸿沟”：** 两个“世界”的内存（系统内存 vs 显存）是物理隔离的。数据必须通过 `cudaMemcpy` 显式来回传递，这是性能瓶颈之一。
3.  **统一内存 (Unified Memory) 是“捷径”：**
    * `cudaMallocManaged` 是初学者的福音。它抹平了 Host 和 Device 的内存界限，让程序员“无感”地在 CPU 和 GPU 之间迁移数据（由驱动程序自动管理）。
    * **代价：** 牺牲了部分性能和控制权。
    * **注意：** 即使使用统一内存，`kernel` 的执行仍是**异步**的。必须使用 `cudaDeviceSynchronize()` 阻塞 Host 线程，等待 GPU 完成计算，才能安全地在 Host 上访问结果。
4.  **性能的“天敌”：**
    * **内存I/O：** `cudaMemcpy` 的调用次数和数据量。
    * **Warp Divergence (线程束分化)：** 一个 Warp (32个线程) 必须执行相同的指令。如果 Warp 内的线程因 `if-else` 走向不同分支，它们将被**串行化**（一个分支执行完，另一个分支再执行），并行性大打折扣。
    * **Block Size 设置：** Block 大小（一个 Block 内的线程数）应**始终是 32 的倍数**，以确保 Warp 被充分利用。

---

## 1. 核心概念：CPU-GPU 异构计算

CUDA 编程模型是一种**异构计算 (Heterogeneous Computing)** 模型，它假定系统包含两类处理器：

* **Host (主机):** CPU，负责串行逻辑和任务调度。
* **Device (设备):Details:** GPU，负责并行计算。

它们通过 PCIe 总线连接。

| 特性 | Host (CPU) | Device (GPU) |
| :--- | :--- | :--- |
| **核心数量** | 少（几核到几十核） | 多（几百到几千核） |
| **设计目标** | 低延迟、强逻辑 | 高吞吐、强计算 |
| **适合任务** | 控制密集型、串行任务 | 数据密集型、并行任务 |
| **线程** | 重量级（上下文切换开销大） | 轻量级（切换开销极小） |

## 2. CUDA 经典执行流程 (5步法)

一个典型的（不使用统一内存的）CUDA 程序执行流程如下：

1.  **分配内存：**
    * 在 Host (CPU) 上分配内存 (`malloc`)。
    * 在 Device (GPU) 上分配内存 (`cudaMalloc`)。
2.  **拷贝数据 (H→D)：**
    * 将 Host 上的输入数据拷贝到 Device 的显存中 (`cudaMemcpyHostToDevice`)。
3.  **执行 Kernel (GPU)：**
    * CPU 调用 **Kernel (核函数)** (`myKernel<<<grid, block>>>`)，在 GPU 上启动大量线程执行并行计算。
4.  **拷贝结果 (D→H)：**
    * 将 Device 上的计算结果拷贝回 Host 内存中 (`cudaMemcpyDeviceToHost`)。
5.  **释放内存：**
    * 释放 Host 内存 (`free`)。
    * 释放 Device 内存 (`cudaFree`)。

## 3. Kernel 与函数类型限定词

**Kernel (核函数)** 是在 GPU 上并行执行的 C++ 函数。

* **声明：** 使用 `__global__` 限定词。
* **返回类型：** 必须是 `void`。
* **调用：** 只能从 Host 端调用，使用 `<<<...>>>` 执行配置。
* **特性：** 调用是**异步**的。Host 调用 Kernel 后会立即执行下一条指令，不会等待 Kernel 执行完毕。

| 限定词 | 执行位置 | 调用位置 | 作用 |
| :--- | :--- | :--- | :--- |
| `__global__` | Device (GPU) | Host (CPU) | 定义一个 Kernel（核函数） |
| `__device__` | Device (GPU) | Device (GPU) | 只能在 GPU 内部调用的函数 |
| `__host__` | Host (CPU) | Host (CPU) | 只能在 CPU 上调用的函数 (默认) |

> `__host__ __device__` 组合：表示该函数会被编译两次，既可以在 Host 端调用，也可以在 Device 端调用。

## 4. 核心中的核心：线程层次结构 (Grid > Block > Thread)

这是 CUDA 最重要、也最容易混淆的概念。CUDA 将逻辑线程组织为两层结构：

1.  **Grid (线程网格):** 一次 Kernel 启动的所有线程的**集合**。
2.  **Block (线程块):** Grid 被划分为一个或多个 Block。
3.  **Thread (线程):** Block 被划分为一个或多个 Thread。

**关键关系：**
* **Grid 由 Block 组成。**
* **Block 由 Thread 组成。**
* **同一个 Block 内的线程** 可以通过 **Shared Memory (共享内存)** 高效通信，也可以进行同步 (`__syncthreads()`)。
* **不同 Block 之间的线程** 无法直接通信，也不能保证执行顺序。

### 4.1. 线程的“坐标”：内置变量

在 Kernel 内部，每个线程都可以通过**内置变量**来知道自己的“身份”：

* `dim3 gridDim`: 网格的维度 (包含多少个 Block)。
* `dim3 blockDim`: 线程块的维度 (包含多少个 Thread)。
* `dim3 blockIdx`: 当前 Block 在 Grid 中的 (x, y, z) 索引。
* `dim3 threadIdx`: 当前 Thread 在 Block 中的 (x, y, z) 索引。

### 4.2. 必会模式：计算全局线程 ID

在实际编程中，我们通常需要一个**全局唯一的线程 ID**，来确定当前线程应该处理哪个数据。

#### 模式一：1D 向量加法

假设处理一个 1D 向量 `data[N]`。
```c++
// Kernel 定义
__global__ void add(float* x, float* y, float* z, int N) {
    // 1. 计算全局线程 ID
    int index = blockIdx.x * blockDim.x + threadIdx.x;

    // 2. 边界检查 (防止越界)
    if (index < N) {
        z[index] = x[index] + y[index];
    }
}

// Host 端调用
int N = 1 << 20;
int threadsPerBlock = 256;
// 向上取整，确保所有 N 个元素都被覆盖
int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;

add<<<blocksPerGrid, threadsPerBlock>>>(d_x, d_y, d_z, N);
````

#### 模式二：2D 矩阵加法 (MatAdd)

假设处理一个 2D 矩阵 `data[N][N]`。

C++

```
__global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N]) {
    // 1. 计算全局的 x 和 y 坐标
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;

    // 2. 边界检查
    if (i < N && j < N) {
        C[i][j] = A[i][j] + B[i][j];
    }
}

// Host 端调用 (假设使用 16x16 的 Block)
dim3 threadsPerBlock(16, 16);
dim3 numBlocks((N + 15) / 16, (N + 15) / 16);
MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
```

#### 模式三：Grid-Stride Loop (网格跨步循环)

这是一个**更健壮、更常用**的模式，用于处理超大数据量。它将“线程”与“数据”解耦。

C++

```
__global__ void add(float* x, float * y, float* z, int n) {
    // 计算全局索引
    int index = threadIdx.x + blockIdx.x * blockDim.x;
    // 计算总步长（整个网格中的线程总数）
    int stride = blockDim.x * gridDim.x;

    // 线程以 stride 为步长，“跳跃”着处理数据
    // 即使只启动了 1024 个线程，也能处理 100 万个元素
    for (int i = index; i < n; i += stride) {
        z[i] = x[i] + y[i];
    }
}
```

## 5. 逻辑到物理的映射：SM 与 Warp

- **SM (Streaming Multiprocessor，流式多处理器):** GPU 硬件的**物理执行单元**。可以看作是 GPU 上的“迷你 CPU”。
    
- **映射关系：**
    
    1. 一个 **Block**（线程块）只会被调度到**一个 SM** 上执行。
        
    2. 一个 **SM** 可以同时并发执行**多个 Block**（只要 SM 上的资源，如寄存器、共享内存足够）。
        
- **Warp (线程束):**
    
    - SM 执行调度的**基本单元**。
        
    - 一个 Warp 包含 **32 个线程**。
        
    - SM 采用 **SIMT (Single-Instruction, Multiple-Thread)** 架构。
        
    - 一个 Warp 中的 32 个线程在同一时刻**必须执行完全相同的指令**。
        
- **Warp Divergence (线程束分化):**
    
    - 如果在一个 Warp 内，线程 A 执行 `if` 分支，线程 B 执行 `else` 分支。
        
    - GPU 会先执行 `if` 分支（线程 B 空等），然后再执行 `else` 分支（线程 A 空等）。
        
    - 这会导致**性能急剧下降**。应尽量避免在 Warp 内出现分支。
        
- **性能推论：**
    
    - `blockDim` (线程块大小) 应始终设置为 **32 的倍数**（如 128, 256, 512），以确保所有 Warp 都是满载的。
        

## 6. CUDA 内存模型

|**内存类型**|**作用域**|**访问速度**|**特点**|
|---|---|---|---|
|**Registers**|单个 Thread|极快|线程私有，用于局部变量|
|**Local Memory**|单个 Thread|慢|线程私有，当寄存器不够时，数据会溢出到这里|
|**Shared Memory**|**整个 Block**|极快 (L1 Cache)|块内所有线程共享，是优化的关键|
|**Global Memory**|**整个 Grid**|慢 (显存)|所有线程、Host 均可访问（通过 `cudaMemcpy`）|
|**Constant Memory**|整个 Grid|快 (有缓存)|只读，适用于所有线程都访问的常量|

> 性能优化的本质： 减少对慢速 Global Memory 的访问，
> 
> 尽可能利用快速 Shared Memory 来缓存和复用数据。
> 
> (文章中的矩阵乘法就是“naive”实现，它没有使用 Shared Memory，每个线程都从 Global Memory 中读取了大量数据，性能很差。)

## 7. 编程实践：API 与模式

### 模式一：显式内存管理 (经典模式)

- **分配 (Device):** `cudaError_t cudaMalloc(void** devPtr, size_t size);`
    
- **释放 (Device):** `cudaError_t cudaFree(void* devPtr);`
    
- **拷贝 (H↔D):** `cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind);`
    
    - `kind`: `cudaMemcpyHostToDevice`, `cudaMemcpyDeviceToHost`
        

### 模式二：统一内存 (Unified Memory, 现代模式)

> CUDA 6.0+ 引入

- **分配 (Managed):** `cudaError_t cudaMallocManaged(void **devPtr, size_t size);`
    
- **特性：**
    
    - 分配一块“托管内存”，Host 和 Device 均可访问。
        
    - CUDA 驱动程序会**自动**在需要时（例如 Kernel 启动或 Host 访问时）在系统内存和显存之间**迁移数据**。
        
    - 极大简化了编程，**无需手动 `cudaMemcpy`**。
        
- **必须的同步：**
    
    - `cudaError_t cudaDeviceSynchronize();`
        
    - 由于 Kernel 启动是异步的，Host 在调用 Kernel 后会立即继续执行。
        
    - 如果 Host 马上访问托管内存中的结果，此时 GPU 可能还没算完。
        
    - 因此，在 Host 访问结果前，**必须调用 `cudaDeviceSynchronize()`** 来阻塞 Host，直到 GPU 完成所有已提交的任务。