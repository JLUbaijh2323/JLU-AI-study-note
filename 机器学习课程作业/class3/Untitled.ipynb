{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-17-f19217ad2443>, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-f19217ad2443>\"\u001b[1;36m, line \u001b[1;32m45\u001b[0m\n\u001b[1;33m    user = sorted(set(u_id for u_id,_,_,_ in data))# set是无序列表，可以去重 通过sorted函数对去重后的id进行升序排序\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import pickle#引入pickle库 实现文本文件与二进制文件之间转化\n",
    "#pt1 转换数据\n",
    "data=[]#定义data列表 以便后面保存嵌套列表\n",
    "with open(\"class2/ML100k.data\",\"r\") as file:\n",
    "    for line in file:#对数据逐行迭代\n",
    "        u_id,f_id,rate,time = map(int,line.strip().split())#将每一行的字符串去除杂乱符号并分割 分割后分别赋值给四个变量\n",
    "        #尝试使用新学会的strip和split默认参数用法：不传参则去掉默认符号，按字符串分割\n",
    "        data.append([u_id,f_id,rate,time])#将四个变量作为一维数组传入data数组中，作为第一条数据（data可视为二维数组）\n",
    "print(f\"前10条：{data[:10]}\")#测试数据\n",
    "\n",
    "with open(\"class2/data.pickle\",\"wb\")as file:\n",
    "    pickle.dump(data,file)#将处理好的数据转化为二进制存入pickle文件\n",
    "\n",
    "#pt2 过滤数据 （目标：1/删除少于五个电影评分的不活跃用户 2/删除被少于5个用户评分的冷门电影）\n",
    "def filter(data,key=5):\n",
    "    while(True):\n",
    "        #统计用户评价次数和电影被评价次数\n",
    "        user_count ={}#通过字典（key-value对应 用户-对应值）模拟计数器\n",
    "        film_count ={}\n",
    "        for u_id,f_id,rate,time in data:#对data中的每行数据迭代 并拆包赋值给四个变量\n",
    "            user_count[u_id] = user_count.get(u_id,0)+1#如果user_count存在u_id这个key，则更新value为对应的value+1\n",
    "            film_count[f_id] = film_count.get(f_id,0)+1#逻辑同上\n",
    "        new_data =[]#定义新列表，记录统计之后的新数据\n",
    "        #筛选数据(逐行迭代，如果用户评分次数和电影被评分次数同时大于key则存入新数据)\n",
    "        for u_id,f_id,rate,time in data:\n",
    "            if user_count[u_id] >=key and film_count[f_id] >=key:\n",
    "                new_data.append([u_id,f_id,rate,time])\n",
    "        #这里会发现存在一个很难受的bug：当某些数据被删除后，会导致某些电影的被评价次数降低，这样的话依旧不符合要求；同理也会造成某些用户评价次数降低\n",
    "        #所以需要不断迭代，直到新数据和处理前数据相同：表明数据已经稳定\n",
    "        if len(new_data) ==len(data):\n",
    "            break;\n",
    "        data = new_data\n",
    "    return data\n",
    "\n",
    "#对我们的数据使用我们定义的函数进行处理\n",
    "f_data =filter(data,5)\n",
    "print(f\"过滤后的数据共有：{len(f_data)}条\")\n",
    "\n",
    "#pt3 统计数据\n",
    "#统计指标：\n",
    "#一 1/用户数量 2/电影数量 3/评分数量 二 1/稀疏度：评分数量/（用户数*电影数）） \n",
    "#三 1/某用户的平均评分(从小到大排序) 2/某个电影的平均评分（从小到大排序） 3/全部评分的平均评分\n",
    "#四 统计所有评分中1-5分的分布情况（每个分数有多少条评分记录）\n",
    "#五 将用户和电影分别重新从0标号\n",
    "    user = sorted(set(u_id for u_id,_,_,_ in data))# set是无序列表，可以去重 通过sorted函数对去重后的id进行升序排序\n",
    "    film = sorted(set(f_id for _,f_id,_,_ in data))\n",
    "    rating= [rate for _,_,rate,_ in data]#评分并不需要去重\n",
    "    u_sum = len(user)\n",
    "    f_sum = len(film)\n",
    "    r_sum = len(rating)\n",
    "    sparsity = r_sum/(f_sum*u_sum)\n",
    "    #第一二部分处理完成\n",
    "    user_avgr ={}\n",
    "    u_count =0\n",
    "    f_avgr ={}\n",
    "    f_count =0\n",
    "    r_count =0\n",
    "    for u in user:#使用去重后的用户列表进行迭代\n",
    "        for u_id,_,r,_ in data if u_id ==u:\n",
    "            u_count =u_count+1\n",
    "        u_avgr[u] = [(u_acgr.get(u,0)+r)/u_count for u_id,_,r,_ in data if u_id ==u]#对data中的数据逐行迭代，如果用户_id和我们迭代的u相同，则将将u的value+r\n",
    "    for f in film:\n",
    "        for _,f_id,r,_ in data if f_id ==f:\n",
    "            f_count =f_count+1\n",
    "        f_avgr[f] = [(f_avgr.get(u,0))f_count+r for _,f_id,r,_ in data if f_id ==f]\n",
    "    for r in rating:\n",
    "        r_count+=r\n",
    "    r_avg=r_count/r_sum\n",
    "    #第三部分处理完成\n",
    "    rate_level={1:0,2:0,3:0,4:0,5:0}\n",
    "    for r in rating:\n",
    "        rate_level[r]=rate_level.get(r,0)+1\n",
    "    #第四部分处理完成\n",
    "    new_u_count = 0\n",
    "    new_f_count = 0\n",
    "    new_user ={}\n",
    "    new_film ={}\n",
    "    for u in user:\n",
    "        new_user[new_u_count]=u\n",
    "        new_u_count+=1\n",
    "    for f in film:\n",
    "        new_film[new_f_count]=f\n",
    "        new_f_count+=1\n",
    "    #第五部分处理完成\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:15<00:00, 641.18it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:1309: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk 统计量: 0.999517023563385\n",
      "p 值: 0.009410574100911617\n"
     ]
    }
   ],
   "source": [
    "# 1. 导入所需库\n",
    "import numpy as np  # 数值计算库，用于生成随机数、数组操作等\n",
    "import matplotlib.pyplot as plt  # 绘图库，用于绘制误差分布直方图\n",
    "from tqdm import tqdm  # 进度条库，让循环过程更直观（大循环时看进度很方便）\n",
    "from sklearn.linear_model import LinearRegression  # 线性回归模型，用于拟合数据\n",
    "from sklearn.metrics import mean_squared_error  # 均方误差函数，评估模型预测误差\n",
    "from scipy.stats import shapiro  # Shapiro-Wilk 检验，用于判断数据是否符合正态分布\n",
    "\n",
    "# 2. 定义生成**训练数据**的函数\n",
    "def generate_data(n_samples):\n",
    "    # 生成特征 X：均值为 0、标准差为 1 的正态分布随机数，形状是 (n_samples, 1)\n",
    "    X = np.random.normal(0, 1, (n_samples, 1))\n",
    "    # 生成标签 y：真实关系是 “y = 3X + 噪声”，噪声也是正态分布（均值 0、标准差 1）\n",
    "    y = 3 * X + np.random.normal(0, 1, (n_samples, 1))\n",
    "    return X, y  # 返回生成的训练特征和标签\n",
    "\n",
    "# 3. 定义生成**测试数据**的函数\n",
    "def generate_data_test(n_samples):\n",
    "    # 生成特征 X：同样是均值 0、标准差 1 的正态分布（和训练集 X 分布一致，保证可比性）\n",
    "    X = np.random.normal(0, 1, (n_samples, 1))\n",
    "    # 生成标签 y：真实关系是 “y = 3X²”（二次关系，用来测试**线性模型**的泛化能力）\n",
    "    y = 3 * X ** 2\n",
    "    return X, y  # 返回生成的测试特征和标签\n",
    "\n",
    "# 4. 定义“训练模型并计算误差”的函数\n",
    "def calculate_error(X_train, X_test, y_train, y_test):\n",
    "    model = LinearRegression()  # 实例化线性回归模型\n",
    "    model.fit(X_train, y_train)  # 用训练数据拟合模型（学“y=3X+噪声”的线性规律）\n",
    "    \n",
    "    y_train_p = model.predict(X_train)  # 用模型预测**训练集**结果（看拟合效果）\n",
    "    y_test_p = model.predict(X_test)    # 用模型预测**测试集**结果（看泛化效果）\n",
    "    \n",
    "    t_error = mean_squared_error(y_train, y_train_p)  # 计算训练集均方误差\n",
    "    g_error = mean_squared_error(y_test, y_test_p)    # 计算测试集均方误差\n",
    "    return t_error, g_error  # 返回训练误差、测试误差\n",
    "\n",
    "# 5. 实验参数设置\n",
    "n_samples = 10000  # 每个数据集的样本数量\n",
    "n_exp = 10000      # 重复实验的次数（多次实验看误差的“分布规律”）\n",
    "g_errors = []      # 用于存储每次实验的**测试误差**\n",
    "\n",
    "# 6. 重复实验循环（tqdm包裹循环，显示进度条）\n",
    "for _ in tqdm(range(n_exp), desc=\"Running experiments\"):\n",
    "    X, y = generate_data(n_samples)        # 生成1次训练数据\n",
    "    X_test, y_test = generate_data_test(n_samples)  # 生成1次测试数据\n",
    "    t_error, g_error = calculate_error(X, X_test, y, y_test)  # 计算本次的训练、测试误差\n",
    "    g_errors.append(g_error)  # 把本次测试误差存入列表\n",
    "\n",
    "# 7. 可视化测试误差的分布\n",
    "plt.hist(g_errors, bins=30, density=True, alpha=0.6)  # 绘制直方图（bins=分箱数；density=密度模式；alpha=透明度）\n",
    "plt.show()  # 显示图形\n",
    "\n",
    "# 8. 正态性检验（Shapiro-Wilk 检验，判断误差是否符合正态分布）\n",
    "stat, p = shapiro(g_errors)\n",
    "print(\"Shapiro-Wilk 统计量:\", stat)\n",
    "print(\"p 值:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle#引入pickle库 实现文本文件与二进制文件之间转化\n",
    "#pt1 转换数据\n",
    "data=[]#定义data列表 以便后面保存嵌套列表\n",
    "with open(\"class2/ML100k.data\",\"r\") as file:\n",
    "    for line in file:#对数据逐行迭代\n",
    "        u_id,f_id,rate,time = map(int,line.strip().split())#将每一行的字符串去除杂乱符号并分割 分割后分别赋值给四个变量\n",
    "        #尝试使用新学会的strip和split默认参数用法：不传参则去掉默认符号，按字符串分割\n",
    "        data.append([u_id,f_id,rate,time])#将四个变量作为一维数组传入data数组中，作为第一条数据（data可视为二维数组）\n",
    "print(f\"前10条：{data[:10]}\")#测试数据\n",
    "\n",
    "with open(\"class2/data.pickle\",\"wb\")as file:\n",
    "    pickle.dump(data,file)#将处理好的数据转化为二进制存入pickle文件\n",
    "\n",
    "#pt2 过滤数据 （目标：1/删除少于五个电影评分的不活跃用户 2/删除被少于5个用户评分的冷门电影）\n",
    "def filter(data,key=5):\n",
    "    while(True):\n",
    "        #统计用户评价次数和电影被评价次数\n",
    "        user_count ={}#通过字典（key-value对应 用户-对应值）模拟计数器\n",
    "        film_count ={}\n",
    "        for u_id,f_id,rate,time in data:#对data中的每行数据迭代 并拆包赋值给四个变量\n",
    "            user_count[u_id] = user_count.get(u_id,0)+1#如果user_count存在u_id这个key，则更新value为对应的value+1\n",
    "            film_count[f_id] = film_count.get(f_id,0)+1#逻辑同上\n",
    "        new_data =[]#定义新列表，记录统计之后的新数据\n",
    "        #筛选数据(逐行迭代，如果用户评分次数和电影被评分次数同时大于key则存入新数据)\n",
    "        for u_id,f_id,rate,time in data:\n",
    "            if user_count[u_id] >=key and film_count[f_id] >=key:\n",
    "                new_data.append([u_id,f_id,rate,time])\n",
    "        #这里会发现存在一个很难受的bug：当某些数据被删除后，会导致某些电影的被评价次数降低，这样的话依旧不符合要求；同理也会造成某些用户评价次数降低\n",
    "        #所以需要不断迭代，直到新数据和处理前数据相同：表明数据已经稳定\n",
    "        if len(new_data) ==len(data):\n",
    "            break;\n",
    "        data = new_data\n",
    "    return data\n",
    "\n",
    "#对我们的数据使用我们定义的函数进行处理\n",
    "f_data =filter(data,5)\n",
    "print(f\"过滤后的数据共有：{len(f_data)}条\")\n",
    "\n",
    "#pt3 统计数据\n",
    "#统计指标：\n",
    "#一 1/用户数量 2/电影数量 3/评分数量 二 1/稀疏度：评分数量/（用户数*电影数）） \n",
    "#三 1/某用户的平均评分(从小到大排序) 2/某个电影的平均评分（从小到大排序） 3/全部评分的平均评分\n",
    "#四 统计所有评分中1-5分的分布情况（每个分数有多少条评分记录）\n",
    "#五 将用户和电影分别重新从0标号\n",
    "\n",
    "def sta(data):\n",
    "    # 第一二部分: 数量统计和稀疏度\n",
    "    user_ids = sorted(list(set(u_id for u_id,_,_,_ in data)))# set是无序列表，可以去重 通过sorted函数对去重后的id进行升序排序\n",
    "    film_ids = sorted(list(set(f_id for _,f_id,_,_ in data)))\n",
    "    rating= [rate for _,_,rate,_ in data]#评分并不需要去重\n",
    "    u_sum = len(user_ids)\n",
    "    f_sum = len(film_ids)\n",
    "    r_sum = len(rating)\n",
    "    sparsity = r_sum/(f_sum*u_sum)\n",
    "    #第一二部分处理完成\n",
    "    user_rating_sums = {}\n",
    "    user_rating_counts = {}\n",
    "    film_rating_sums = {}\n",
    "    film_rating_counts = {}\n",
    "    total_rating_sum = 0\n",
    "    \n",
    "    # 修正后的第三部分：使用一次遍历统计评分总和和次数\n",
    "    for u_id, f_id, rate, _ in data:\n",
    "        user_rating_sums[u_id] = user_rating_sums.get(u_id, 0) + rate\n",
    "        user_rating_counts[u_id] = user_rating_counts.get(u_id, 0) + 1\n",
    "        film_rating_sums[f_id] = film_rating_sums.get(f_id, 0) + rate\n",
    "        film_rating_counts[f_id] = film_rating_counts.get(f_id, 0) + 1\n",
    "        total_rating_sum += rate\n",
    "    \n",
    "    user_avg_ratings = {u_id: user_rating_sums[u_id] / user_rating_counts[u_id] for u_id in user_rating_sums}\n",
    "    film_avg_ratings = {f_id: film_rating_sums[f_id] / film_rating_counts[f_id] for f_id in film_rating_sums}\n",
    "    all_rating_avg = total_rating_sum / r_sum\n",
    "    #第三部分处理完成\n",
    "\n",
    "    rate_level={1:0,2:0,3:0,4:0,5:0}\n",
    "    for r in rating:\n",
    "        rate_level[r]=rate_level.get(r,0)+1\n",
    "    #第四部分处理完成\n",
    "\n",
    "    # 修正后的第五部分：创建旧ID到新ID的映射\n",
    "    user_map = {}\n",
    "    new_u_count = 0\n",
    "    for u_id in user_ids:\n",
    "        user_map[u_id] = new_u_count\n",
    "        new_u_count += 1\n",
    "    \n",
    "    film_map = {}\n",
    "    new_f_count = 0\n",
    "    for f_id in film_ids:\n",
    "        film_map[f_id] = new_f_count\n",
    "        new_f_count += 1\n",
    "\n",
    "    # 根据映射生成新的数据集\n",
    "    reindexed_data = []\n",
    "    for u_id, f_id, rate, time in data:\n",
    "        new_u_id = user_map[u_id]\n",
    "        new_f_id = film_map[f_id]\n",
    "        reindexed_data.append([new_u_id, new_f_id, rate, time])\n",
    "    #第五部分处理完成\n",
    "\n",
    "    #测试\n",
    "    print(\"---\")\n",
    "    print(\"## 统计结果\")\n",
    "    print(f\"用户数量: {u_sum}\")\n",
    "    print(f\"电影数量: {f_sum}\")\n",
    "    print(f\"评分数量: {r_sum}\")\n",
    "    print(f\"数据稀疏度: {sparsity:.6f}\")\n",
    "    \n",
    "    print(\"\\n用户平均评分(前10):\")\n",
    "    sorted_user_avg = sorted(user_avg_ratings.items(), key=lambda item: item[1])\n",
    "    for u, avg in sorted_user_avg[:10]:\n",
    "        print(f\"  用户 {u}: {avg:.2f}\")\n",
    "\n",
    "    print(\"\\n电影平均评分(前10):\")\n",
    "    sorted_film_avg = sorted(film_avg_ratings.items(), key=lambda item: item[1])\n",
    "    for f, avg in sorted_film_avg[:10]:\n",
    "        print(f\"  电影 {f}: {avg:.2f}\")\n",
    "    \n",
    "    print(f\"\\n所有评分的平均评分: {all_rating_avg:.2f}\")\n",
    "    \n",
    "    print(\"\\n---\")\n",
    "    print(\"## 评分分布\")\n",
    "    for rate, count in sorted(rate_level.items()):\n",
    "        print(f\"  评分 {rate}: {count} 条记录\")\n",
    "\n",
    "    print(\"\\n---\")\n",
    "    print(\"## 数据重新编号\")\n",
    "    print(f\"原用户ID列表(前10): {user_ids[:10]}\")\n",
    "    print(f\"新用户ID列表(前10): {[user_map[u] for u in user_ids[:10]]}\")\n",
    "    print(f\"原电影ID列表(前10): {film_ids[:10]}\")\n",
    "    print(f\"新电影ID列表(前10): {[film_map[f] for f in film_ids[:10]]}\")\n",
    "    print(f\"重新编号后的数据(前10): {reindexed_data[:10]}\")\n",
    "\n",
    "    return reindexed_data\n",
    "\n",
    "reindexed_f_data = sta(f_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
