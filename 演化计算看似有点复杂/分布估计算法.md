### 1. 快速回顾：遗传算法 (GA)

GA 模拟的是达尔文的“适者生存”进化论。

- **核心思想：** 通过“选择”、“交叉”和“变异”来迭代出一个最优解。
    
- **流程：**
    
    1. **初始化：** 随机生成一个种群（一堆解，称为“染色体”）。
        
    2. **评估：** 计算每个解的“适应度”（即这个解有多好）。
        
    3. **选择：** 根据适应度，““优胜劣汰””。适应度高的解（““个体””）有更高概率被选中作为“父代”。
        
    4. **繁殖（GA的核心）：**
        
        - **交叉 (Crossover)：** 随机挑选两个“父代”，将它们的“染色体”（解）进行**““基因重组””**（例如，交换一半），产生“子代”。这是 GA **““利用””**（Exploitation）已有优秀基因的主要方式。
            
        - **变异 (Mutation)：** “子代”的“染色体”有很小概率发生**““基因突变””**（例如，解里的某个值随机变一下）。这是 GA 保持种群多样性、**““探索””**（Exploration）新解的主要方式。
            
    5. **循环：** 新的“子代”种群替代“父代”种群，回到第 2 步，一代代进化，直到找到满意的解。
        
- **关键点：** GA 的“繁殖”操作（交叉/变异）是**““盲目””**的。它只是机械地交换和修改解的片段，**它并不““理解””一个解为什么好**，也不知道解的各个部分之间有什么**关联**。
    

---

### 2. 深入剖析：分布估计算法 (EDA)

EDA 的出现，就是为了解决 GA 的““盲目””繁殖问题。

#### 2.1 EDA 是什么 (What)

分布估计算法（Estimation of Distribution Algorithm）是一种**““用统计建模取代交叉变异””**的进化算法。

- **核心思想：** GA 是让“个体”之间““盲目””繁殖；而 EDA 则是扮演一个““统计学家””的角色。它不再让个体两两配对，而是**““观察””当前所有““优秀个体””**，从它们身上**““学习””**出一个**概率分布模型 (Probabilistic Model)**。
    
- **EDA 的比喻：**
    
    - **GA：** 想培育好马。它把两匹跑得最快的马（父代）抓来配对，生个小马（子代），期待小马能““青出于蓝””。
        
    - **EDA：** 也想培育好马。但它做的是：把**所有**跑得快的马（优秀群体）的基因全部抽出来，放到计算机里分析，**建立一个““冠军马的基因概率模型””**。然后，它**根据这个模型““合成””一批全新的小马**（新种群）。
        

**EDA 的““革命性””在于：** 它用**““从模型中采样”” (Sampling)** 来**完全替代**了 GA 的**““交叉与变异”” (Crossover & Mutation)**。

#### 2.2 EDA 是如何工作的 (How - 流程)

这是 EDA 的重中之重。EDA 的算法流程非常清晰：

1. **初始化 (Initialization):**
    
    - 随机生成一个初始种群 $D(0)$（包含 $N$ 个个体/解）。
        
    - 设置迭代计数器 $t = 0$。
        
2. **评估 (Evaluation):**
    
    - 计算当前种群 $D(t)$ 中**所有**个体的适应度。
        
3. **选择 (Selection):**
    
    - 从种群 $D(t)$ 中，根据适应度**选择**出一个“优秀个体”的子集 $S(t)$。
        
    - （例如：选择适应度排名前 50% 的个体，数量为 $M$，其中 $M \le N$）。
        
4. **概率建模 (Modeling) - EDA的核心**
    
    - 这是 EDA 最关键的一步。
        
    - 算法““观察””**这个优秀子集 $S(t)$。
        
    - **估计**（Learn / Estimate）出一个概率分布模型 $P_t(x)$，这个模型必须能**捕捉到 $S(t)$ 中这些优秀解的共同统计特征**（比如，“在优秀解中，第3位是1的概率是80%”，或者“如果第2位是1，那么第5位也很可能是1”）。
        
5. **生成新种群 (Sampling):**
    
    - **““丢弃””旧种群 $D(t)$ （或只保留极少数最精英个体）。
        
    - 根据上一步学到的概率模型 $P_t(x)$，**““采样””**（Sample）生成 $N$ 个全新的个体。
        
    - 这些新个体““天生””就符合优秀群体 $S(t)$ 的统计规律。
        
    - 这 $N$ 个新个体组成了下一代种群 $D(t+1)$。
        
6. **终止判断 (Termination):**
    
    - 检查是否满足终止条件（例如：达到最大迭代次数？解的质量不再提升？）。
        
    - **如果满足：** 停止，并输出迄今为止找到的最佳解。
        
    - **如果不满足：** $t = t + 1$，然后**返回第 2 步**（评估新种群 $D(t+1)$）。
        

**流程对比：**

- **GA：** 初始化 $\rightarrow$ 评估 $\rightarrow$ 选择 $\rightarrow$ **交叉/变异** $\rightarrow$ 循环
    
- **EDA：** 初始化 $\rightarrow$ 评估 $\rightarrow$ 选择 $\rightarrow$ **建模/采样** $\rightarrow$ 循环
    

#### 2.3 为什么要用 EDA (Why)

我们知道了 GA ““盲目””，EDA ““智能””，但这背后的根本原因是什么？

根本原因：

GA 的“交叉”操作会““破坏（Disrupt）””优良的基因块（““积木块”，Building Blocks）**。

- **““积木块””：** 是指解中那些优良的、短小的基因片段。比如，对于一个问题，最优解是 `11111111`，那么 `11` 可能就是一个“积木块”。
    
- **GA 的““破坏””：** 假设“父代1”是 `11110000`，“父代2”是 `00001111`。它俩都包含 `1111` 这个“积木块”。
    
    - 如果 GA 在中间进行“单点交叉”：`1111|0000` 和 `0000|1111`
        
    - 产生的“子代”可能是：`11111111` (好) 和 `00000000` (差)。
        
    - 这还不错，但如果“积木块”不在一起呢？
        
- **““欺骗性问题”” (Deceptive Problem)：** 假设“父代1”是 `10101010`，“父代2”是 `01010101`，它俩适应度都还行。但**最优解**其实是 `11111111`。
    
    - GA 的交叉操作，无论怎么交叉 `1010...` 和 `0101...`，都**极难**产生 `1111...`。它会不断地““破坏”” `1...1...` 和 `...0...0` 这种潜在的“积木块”。
        
- **EDA 的““优势””：**
    
    - EDA 不做“交叉”。它在“建模”（步骤4）时，会““发现””**优秀个体 $S(t)$ 中存在的**““关联性””**。
        
    - 如果 `10101010` 和 `01010101` 是好的，EDA 的模型会学到“奇数位和偶数位的数总是相反”。
        
    - 如果 `1100` 和 `1110` 是好的，EDA 的模型会学到“第1位和第2位是1的概率非常高”。
        
    - EDA 在“采样”（步骤5）时，是““建设性””**的。它根据模型**““一次性””**生成一个新解，直接保留了那些被学到的““积木块””**，不会在“交叉”过程中被无情地““砍断””。
        

**一句话总结 Why：EDA 用““显式地学习和复现优秀解的统计关联””取代了““隐式地、盲目地拼接优秀解的片段””。**

---

### 3. EDA 的改进与优化（分类）

EDA 的流程是统一的（评估、选择、建模、采样）。那么 EDA 算法之间的区别在哪里？

**答案就在““概率建模””（步骤4）这一步的““复杂度””上。**

EDA 的“改进”和“优化”，指的就是它使用了**““多复杂””的概率模型**来捕捉““积木块””之间的关联性。

#### 类型 1：单元（一元）模型 (Univariate Models)

- **核心假设：** 最简单的假设，**““所有变量（基因位）之间都是相互独立的””**。
    
- **建模：** 它只关心每个位置 $i$ 单独是 $1$ 的概率 $P(x_i)$。模型就是一个简单的概率向量 $p = (p_1, p_2, ..., p_n)$，其中 $p_i$ 是通过统计优秀群体 $S(t)$ 中第 $i$ 位是 1 的频率得来的。
    
- **采样：** 生成新解时，对每一位 $i$，都单独““扔一个（有偏的）硬币””（按 $p_i$ 的概率）来决定是 0 还是 1。
    
- **代表算法：**
    
    - **PBIL** (Population-Based Incremental Learning)
        
    - **UMDA** (Univariate Marginal Distribution Algorithm)
        
- **缺点：** 无法捕捉任何变量““之间””**的联系。如果最优解要求“第1位和第2位必须相同”，PBIL 永远学不会。
    

#### 类型 2：二元模型 (Bivariate Models)

- **核心假设：** 进了一步，**““变量之间最多只存在两两（成对）的依赖关系””**。
    
- **建模：** 模型不再是独立的，而是变成了一个“链” (Chain) 或者“树” (Tree) 的结构。算法会计算所有变量两两之间的“互信息”(Mutual Information)，来构建一个依赖树。
    
- **采样：** 按照这个树（或链）的结构，先确定“根节点”的值，然后根据 $P(x_i | x_j)$（$x_j$ 是 $x_i$ 的““父节点””）来依次确定其他位的值。
    
- **代表算法：**
    
    - **MIMIC** (Mutual Information Maximization for Input Clustering)
        
    - **COMIT** (Combining Optimizer with Mutual Information Trees)
        
- **缺点：** 能捕捉二元关系（如 $x_1$ 和 $x_2$ 总是一起出现），但无法捕捉三元或更多元的复杂关系（如“当 $x_1$ 和 $x_2$ 都是1时，$x_3$ 必须是0”）。
    

#### 类型 3：多元模型 (Multivariate Models)

- **核心假设：** 最高级的假设，**““变量之间可以存在任意复杂的依赖关系””**。
    
- **建模：** 使用最强大的概率图模型——**““贝叶斯网络”” (Bayesian Networks)** 或马尔可夫网络。
    
    - 这非常复杂，算法不仅要学习概率（““参数学习””），还要**““学习网络结构本身””**（““结构学习””），即找出“谁依赖谁”这个图。
        
- **采样：** 从这个复杂的贝叶斯网络中进行采样。
    
- **代表算法：**
    
    - **BOA** (Bayesian Optimization Algorithm)
        
    - **EBNA** (Estimation of Bayesian Network Algorithm)
        
    - **LFDA** (Learnable Factored Distribution Algorithm)
        
- **优点：** 理论上，只要模型足够复杂，它可以捕捉并复现任意复杂的“积木块”，从而解决GA根本无法解决的““高度欺骗性””问题。
    
- **缺点：** **““计算量爆炸””**。学习一个复杂的贝叶斯网络结构本身就是一个 NP-hard 问题。在“建模”（步骤4）上花费的时间可能会非常非常长。
    

### 总结

- **GA** 用“交叉变异”来繁殖，**““盲目””**但**““快速””**。
    
- **EDA** 用“建模采样”来繁殖，**““智能””**但**““有代价””**。
    
- EDA 的**““进化””**，就是其内部“概率模型”**从“独立”到“二元”再到“多元（贝叶斯）”**的进化，以牺牲“建模”的计算代价，来换取捕捉更复杂“积木块”的能力。

# 补充：概率基础
### 1. 万物的起点：随机变量 (Random Variable)

- 是什么？
    
    想象你要解决一个问题，最优解是一个长度为 5 的二进制串，比如 11010。
    
    在 EDA 中，我们把这个解的每一位都看作一个**“随机变量”**。
    
    - 解 $x = (x_1, x_2, x_3, x_4, x_5)$
        
    - $X_1$ 是一个随机变量，它的取值可能是 0 或 1。
        
    - $X_2$ 也是一个随机变量，取值 0 或 1。
        
    - ... 以此类推。
        
- 为什么重要？
    
    “随机变量”就是概率论的““主语””。我们后续所有的讨论，比如“概率是多少？”，“独立吗？”，都是在讨论这些随机变量 $X_i$。
    

### 2. 描述变量：概率分布 (Probability Distribution)

- 是什么？
    
    概率分布就是用来描述一个随机变量取各个值的可能性。
    
- 最简单的分布（一元分布）：
    
    我们只看 $X_1$。它要么是 0，要么是 1。它的分布就是：
    
    - $P(X_1 = 1) = p_1$ （$X_1$ 是 1 的概率）
        
    - $P(X_1 = 0) = 1 - p_1$ （$X_1$ 是 0 的概率）
        
- EDA 的第一层（建模）：
    
    EDA 在“建模”时要做的最简单的事，就是去“估计”这个 $p_1$。
    
    怎么估计？ 这就是“最大似然估计 (MLE)”，但它有个更简单的名字，叫“频率估计”，或者直白点叫“数数”**。
    
    - 假设 EDA 选出了 100 个“优秀个体”（$S(t)$）。
        
    - 它去“数”这 100 个解里，**第 1 位**是 1 的有多少个。
        
    - 如果有 80 个解的 $X_1$ 是 1，那 EDA 就估计 $P(X_1=1) = 80 / 100 = 0.8$。
        
    - 这就是最简单的 EDA 算法 **PBIL** 或 **UMDA** 所做的事情。它为**每一位**都独立地估计了一个概率，得到一个概率向量 $(p_1, p_2, ..., p_n)$。
        

### 3. 描述“关系”：联合概率 (Joint Probability)

- 是什么？
    
    现实世界是复杂的。$X_1$ 和 $X_2$ 之间可能有关联。
    
    - 比如，最优解可能是 `11000` 和 `11100`。你会发现，只要 $X_1$ 是 1， $X_2$ 似乎**也必须**是 1。
        
    - **联合概率 $P(X_1, X_2)$** 就是用来描述“多个变量**同时**取特定值”的概率。
        
    - 例如：$P(X_1=1, X_2=1)$ 是多少？$P(X_1=1, X_2=0)$ 是多少？
        
- EDA 的“终极目标”：
    
    EDA 算法的终极目标，就是想搞清楚整个解 $x = (X_1, X_2, ..., X_n)$ 的联合概率分布 $P(X_1, X_2, ..., X_n)$。
    
    - **为什么？** 因为如果 EDA 知道了这个““神之分布””（即“优秀解”的真正分布），它就可以**直接从这个分布中“采样”**，生成一个完美的、全新的、高质量的解！
        
- 为什么很难？
    
    对于一个长度为 $n$ 的二进制串，这个联合概率分布表需要 $2^n$ 行！如果 $n=100$，这个数字比宇宙中的原子还多。这在计算上是绝对不可能的。
    

**所以，EDA 的全部““花活””，都是在研究如何““近似””这个复杂得要命的 $P(X_1, ..., X_n)$。**

---

### 4. 近似的基础(1)：独立性 (Independence)

- 是什么？
    
    既然 $P(X_1, ..., X_n)$ 太复杂，我们就上““大砍刀””：
    
    最暴力的假设（假设1）： 所有变量 $X_i$ 之间都是**““统计独立””**的。
    
    - “独立”意味着 $X_1$ 是 1 还是 0，对 $X_2$ 毫无影响。
        
    - 用数学语言说：$P(X_1, X_2) = P(X_1) \times P(X_2)$
        
- EDA 的应用（类型1：单元模型）：
    
    如果做了这个“独立性假设”，那个“神之分布” $P(X_1, ..., X_n)$ 就**““退化””**成了：
    
    $P(X_1, ..., X_n) \approx P(X_1) \times P(X_2) \times ... \times P(X_n)$
    
    - **这眼熟吗？** 这就是我们第 2 节里说的 PBIL/UMDA 算法！
        
    - 它放弃了学习变量之间的**任何关系**，只学习每个变量**单独**的概率。
        
    - **优点：** 建模超快（数数就行）。
        
    - **缺点：** 它永远学不会“$X_1$ 和 $X_2$ 必须相同”这种“积木块”。
        

### 5. 近似的基础(2)：条件概率 (Conditional Probability)

- 是什么？
    
    这是概率论的灵魂，也是 EDA 变得“智能”的关键。
    
    - **$P(A | B)$**：“在 **B 已经发生** 的条件下，A 发生的概率”。
        
    - **例子：** $P(X_2=1 | X_1=1)$
        
    - **翻译：** “在第 1 位**已经**是 1 的前提下，第 2 位是 1 的概率是多少？”
        
- 它和“数数”的关系：
    
    这个也能“数”出来！
    
    $P(X_2=1 | X_1=1) = \frac{\text{“X_1=1$ 且 $X_2=1$” 的优秀解数量}}{\text{“$X_1=1$” 的优秀解总数量}}$
    
- 为什么重要？
    
    “条件概率”是描述“依赖关系”的数学语言。
    
    - 如果 $P(X_2=1 | X_1=1) \approx P(X_2=1)$，说明 $X_1$ 对 $X_2$ 没啥影响（接近独立）。
        
    - 如果 $P(X_2=1 | X_1=1) = 0.9$，而 $P(X_2=1) = 0.5$，这说明 $X_1$ 对 $X_2$ **有强烈的正相关**！**这就是 EDA 想找的“积木块”！**
        

---

### 6. 终极武器：链式法则 (Chain Rule)

- 是什么？
    
    链式法则是连接“联合概率”和“条件概率”的桥梁。它说，任何复杂的联合概率，都可以**““拆解””**成一连串条件概率的乘积：
    
    $P(X_1, X_2, X_3) = P(X_1) \times P(X_2 | X_1) \times P(X_3 | X_1, X_2)$
    
- 通俗理解：
    
    “（1, 2, 3 同时发生）的概率”等于：
    
    “（1 发生）的概率” $\times$ “（在 1 发生的前提下，2 发生）的概率” $\times$ “（在 1 和 2 都发生的前提下，3 发生）的概率”。
    
    这在逻辑上是**““恒等””**的，完全没做任何近似！
    
- 问题：
    
    这个“完整”的拆解（如 $P(X_3 | X_1, X_2)$）还是太复杂了，需要“数”的条件组合太多，和第 3 节一样有 $2^n$ 种可能。
    

### 7. EDA 的““智能””：贝叶斯网络 (Bayesian Networks)

我们有“大砍刀”（完全独立，太傻），我们也有“完整版”（链式法则，太复杂）。

EDA 的高级算法（类型2和3），就是在这两者之间**““取折中””**。

- 核心思想：
    
    我们使用链式法则，但要**““简化””**那些条件概率。
    
    - 完整的：$P(X_1, X_2, X_3, X_4) = P(X_1) \times P(X_2 | X_1) \times P(X_3 | X_1, X_2) \times P(X_4 | X_1, X_2, X_3)$
        
    - **EDA 开始““学习””：** 它通过分析数据（优秀个体 $S(t)$），发现 $X_3$ **好像只跟 $X_1$ 有关**，跟 $X_2$ 关系不大。
        
    - **EDA 做出““近似””：** 它假设 $P(X_3 | X_1, X_2) \approx P(X_3 | X_1)$。（这就是**““条件独立””**假设）
        
    - 它又发现 $X_4$ 好像**只跟 $X_2$ 和 $X_3$ 有关**。
        
    - **EDA 做出““近似””：** $P(X_4 | X_1, X_2, X_3) \approx P(X_4 | X_2, X_3)$
        
- 建模结果（贝叶斯网络）：
    
    那个“神之分布” $P(X_1, ..., X_4)$ 就被近似为：
    
    $P_{BOA}(x) \approx P(X_1) \times P(X_2 | X_1) \times P(X_3 | X_1) \times P(X_4 | X_2, X_3)$
    
    这个近似模型，就是一个**““贝叶斯网络””**！它用一个**““图””**（Graph）来表示变量之间“谁依赖谁”：
    
    - $X_1 \rightarrow X_2$
        
    - $X_1 \rightarrow X_3$
        
    - $X_2 \rightarrow X_4$
        
    - $X_3 \rightarrow X_4$
        
- **EDA 类型2 (MIMIC)：** 使用的是一种特殊的、更简单的贝叶斯网络——“树” (Tree) 或“链” (Chain)。
    
- **EDA 类型3 (BOA)：** 试图学习一个更通用的、如图所示的“有向无环图”。
    

---

### 总结：概率论在 EDA 流程中的体现

我们来串一下：

1. **EDA 流程 (How)：** 评估 $\rightarrow$ **选择** $\rightarrow$ **建模** $\rightarrow$ **采样**
    
2. **概率论 (Why)：**
    
    - **选择 (Selection)：** 选出优秀个体 $S(t)$，这是**““原始数据””**。
        
    - **建模 (Modeling)：**
        
        - **目标：** 近似“神之分布” $P(X_1, ..., X_n)$。
            
        - **方法1 (PBIL/UMDA)：** 假设**““独立性””**，$P \approx \prod P(X_i)$。
            
        - **方法3 (BOA)：** 假设**““条件独立性””**，构建一个**““贝叶斯网络””** $P_{BOA}(x)$。
            
        - **具体操作：** 无论哪种模型，都是用 $S(t)$ 作为数据，通过**““数数””**（最大似然估计）来填满模型中的概率值（如 $P(X_i)$ 或 $P(X_i | \text{Parents}(X_i))$）。
            
    - **采样 (Sampling)：**
        
        - **目标：** 从学到的模型 $P(x)$ 中生成新个体。
            
        - **方法 (PBIL/UMDA)：** 很简单。对每一位 $i$，都按 $P(X_i)$ 独立“扔硬币”。
            
        - **方法 (BOA)：** 必须按照贝叶斯网络的“依赖顺序”采样。先采样 $X_1$，然后才能根据 $X_1$ 的结果去采样 $X_2$ 和 $X_3$，最后根据 $X_2, X_3$ 的结果去采样 $X_4$。
            

**一句话总结：EDA 就是一个““统计学家””，它通过（选择）观察数据，（建模）建立一个关于““好解””长什么样的概率模型（从简单独立到复杂贝叶斯），最后（采样）让这个模型自己““写作业””（生成新解）。**